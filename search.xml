<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2Fnanyi-air.github.io%2F2020%2F01%2F14%2Ftest%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[linux中node的安装]]></title>
    <url>%2Fnanyi-air.github.io%2F2020%2F01%2F14%2Flinux%E4%B8%ADnode%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[1.打开node官网下载对应版本的文件Node下载地址 解压命令：tar xvJf ***.tar.xz 2.解压文件到本地注意:将文件复制到linux的opt目录下,不然可能创建软连接后无法执行3.创建对应的软连接12sudo ln -s /opt/node-xx/npm /usr/bin/npmsudo ln -s /opt/node-xx/node /usr/bin/node 4.测试是否安装成功12node -vnpm -v]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>node</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程锁]]></title>
    <url>%2Fnanyi-air.github.io%2F2020%2F01%2F14%2F%E8%BF%9B%E7%A8%8B%E9%94%81%2F</url>
    <content type="text"><![CDATA[进程锁框架1.介绍该代码适用于多进程中需要保证每个进程独立运行,并且某段代码只需要随机任意一个进程执行一次即可的情景. 原理:当多个进程都进入该段代码.某一个进程会创建文件并写入自己的pid,其他进程过来发现文件已创建就会将自己进程杀死,以实现多进程只执行一次该代码的逻辑 2.进程锁框架代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import osimport timeclass ProcessLock(object): """ only one process can run, others will wait until that done. with ProcessLock(filename='tmp.lock'): do something """ def __init__(self, filename=None): self.filename = filename or 'alockfile.lock' @property def status(self): pid = str(os.getpid()) i = 0 while i &lt;= 3: try: # "x"模式创建文件并写入,如果文件已存在会报错 with open(self.filename, 'x') as f: f.write(pid) return True except FileExistsError: try: with open(self.filename) as f: epid = f.read() if epid.isdigit(): epid = int(epid) try: os.kill(epid, 0) except ProcessLookupError: # 用于删除文件,如果文件是一个目录则返回一个错误 os.unlink(self.filename) else: os.unlink(self.filename) except Exception: pass finally: time.sleep(1) i += 1 return Falseif __name__ == '__main__': if ProcessLock().status: print('ok')]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>线程与进程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多线程]]></title>
    <url>%2Fnanyi-air.github.io%2F2020%2F01%2F14%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[多线程的使用1.创建多线程12345678910111213# target=?为任务名(方法名)def main(start=None, end=None): q = Queue(maxsize=100) p = threading.Thread(target=produce, args=(q, start, end)) p.start() for i in range(10): w = threading.Thread(target=worker, args=(q,)) w.setDaemon(True) w.start() # queue.join()阻塞等待队列中任务全部处理完毕，配合queue.task_done使用 p.join() q.join() 2.创建任务123456# queue.put()为放入数据def produce(queue: Queue, start=None, end=None): for i in src_object: if i['id'] in exist_ids: continue queue.put(i) 12345# queue.get()为获取数据def worker(queue): i = queue.get() # 当get方法调用数据完成后就可以使用task_done方法让调用queue.join()的线程返回,证明任务执行完毕 queue.task_done() 3.小结queue.put()：申请获得互斥锁，获得后，如果队列未满，则向队列中添加数据，并通知notify其它阻塞的某个线程，唤醒等待获取require互斥锁。如果队列已满，则会wait等待。最后处理完成后释放互斥锁。其中还有阻塞block以及非阻塞，超时等逻辑。 queue.get()：从队列中获取任务，并且从队列中移除此任务。首先尝试获取互斥锁，获取成功则队列中get任务，如果此时队列为空，则wait等待生产者线程添加数据。get到任务后，会调用self.not_full.notify()通知生产者线程，队列可以添加元素了。最后释放互斥锁。 queue.task_done()：消费者线程从队列中get到任务后，任务处理完成，当所有的队列中的任务处理完成后，会使调用queue.join()的线程返回，表示队列中任务以处理完毕。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>线程与进程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[错误日志以及邮件]]></title>
    <url>%2Fnanyi-air.github.io%2F2020%2F01%2F14%2F%E9%94%99%E8%AF%AF%E6%97%A5%E5%BF%97%E4%BB%A5%E5%8F%8A%E9%82%AE%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[错误日志以及邮件报警1.介绍错误日志分为不同等级,error等级发邮件,info打印控制台以及写入文件 邮件发送为5分钟一次,将错误内容上传到kafka然后5分钟消费一次,统一发送 2.应用(1)Config中配置(config.py)12345678910111213mail_topic = 'mail_service' # kafkatopicmail_title = 'xxx' # 邮件标题send_mail = False # 是否发送邮件logger_level = logging.DEBUG # 日志级别kafka_producer_config = dict( bootstrap_servers=[服务器IP:端口号], security_protocol='SASL_SSL', # 安全协议 sasl_mechanism="PLAIN", # 机制 ssl_context=context, sasl_plain_username='用户名', sasl_plain_password='密码', # api_version=(1, 10), ) (2)日志代码(handler.py)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164# coding: utf-8import osimport loggingimport datetimefrom logging import StreamHandler, Formatter, FileHandlerfrom config import Configfrom common.mail_producer import MailHandler, MailProducerlogging_format = "[%(asctime)s]-[%(levelname)s] &lt;-%(filename)s-%(funcName)s-line %(lineno)s&gt;: %(message)s"datefmt = Nonelevel = Config.logger_levelmail_handler = Noneclass TimeFileHandler(FileHandler): """ A handler class which writes formatted logging records to files with date. """ def __init__(self, filename, mode='a', encoding=None, delay=False, when='d'): """ :param filename: test :param mode: :param encoding: :param delay: :param when: create log file with time format, y: year m: month d: day w: week H: hour M: minute """ self.date = self.__get_current_date(when) self.filename = filename self.mode = mode self.encoding = encoding self.delay = delay self.when = when filename = f'&#123;filename&#125;_&#123;self.date&#125;.log' self.baseFilename = os.path.abspath(filename) FileHandler.__init__(self, filename, mode, encoding, delay) @staticmethod def __get_current_date(when): now = datetime.datetime.now() if when == 'd': date = now.strftime('%Y-%m-%d') elif when == 'w': w = int(now.strftime('%w')) start = datetime.datetime(now.year, now.month, now.day) - datetime.timedelta(days=w-1) date = start.strftime('%Y-%m-%d') elif when == 'm': start = datetime.datetime(now.year, now.month, 1) date = start.strftime('%Y-%m-%d') elif when == 'y': start = datetime.datetime(now.year, 1, 1) date = start.strftime('%Y-%m-%d') elif when == 'H': date = now.strftime('%Y-%m-%d_%H') elif when == 'M': date = now.strftime('%Y-%m-%d_%H-%M') else: date = now.strftime('%Y-%m-%d') return date def emit(self, record): date = self.__get_current_date(self.when) if date != self.date: self.date = date filename = f'&#123;self.filename&#125;_&#123;self.date&#125;.log' self.baseFilename = os.path.abspath(filename) self.stream = self._open() FileHandler.emit(self, record) def _open(self): """ Open the current base file with the (original) mode and encoding. Return the resulting stream. """ return open(self.baseFilename, self.mode, encoding=self.encoding)def get_stream_handler(): # 屏幕显示 stream_handler = StreamHandler() stream_handler.setLevel(level) stream_handler.setFormatter( Formatter(logging_format, datefmt), ) return stream_handlerdef get_file_handler(filename): file_handler = TimeFileHandler(filename) file_handler.setLevel(level) file_handler.setFormatter( Formatter(logging_format, datefmt), ) return file_handlerdef get_mail_handler(): # error级别的log发送邮件 global mail_handler if not mail_handler: mail_handler = MailHandler(topic=Config.mail_topic, title=Config.mail_title, **Config.kafka_producer_config) mail_handler.setLevel(logging.ERROR) mail_handler.setFormatter( Formatter(logging_format, datefmt), ) return mail_handlerdef get_logger(name='root', send_mail=False): """ :param name: logger 名字 :param send_mail: 是否在发生严重错误时候发送邮件 :param level: 记录log级别 :return: """ base_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'logs') if not os.path.isdir(base_dir): os.mkdir(base_dir) filename = os.path.join(base_dir, name) logger = logging.getLogger(name) logger.setLevel(level) # 屏幕显示 stream_handler = get_stream_handler() logger.addHandler(stream_handler) # 文件记录 file_handler = get_file_handler(filename) logger.addHandler(file_handler) if send_mail: logger.addHandler(get_mail_handler()) return loggerroot_logger = get_logger('root', send_mail=True)consumer_logger1 = get_logger('consumer1', send_mail=True)consumer_logger2 = get_logger('consumer2', send_mail=True)consumer_logger3 = get_logger('consumer3', send_mail=True)spider_logger = get_logger('spider', send_mail=True)mail_producer = MailProducer( topic=Config.mail_topic, title=Config.mail_title, send_mail=Config.send_mail, **Config.kafka_producer_config,)if __name__ == '__main__': consumer_logger1.critical('实时任务发生错误') mail_producer.send('手动发送邮件10000', code=10000) mail_producer.send('手动发送邮件10000', code=10000) mail_producer.send('手动发送邮件10001', code=10001) (3)邮件发送123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147import jsonimport sslimport osimport uuidfrom collections import OrderedDictimport asynciofrom aiokafka import AIOKafkaConsumerfrom smtplib import SMTP_SSLfrom email.mime.text import MIMETextimport loggingimport tracebackfrom logging import FileHandler, Formatter, StreamHandlerLOG_FILE = 'mail.log'# 定时5分钟发一次TIME_DELTA = 5 * 60context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)context.verify_mode = ssl.CERT_REQUIREDcafile = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) + "/ca-cert"context.load_verify_locations(cafile)kafka_config = dict( bootstrap_servers=[服务器IP:端口号], security_protocol='SASL_SSL', # 安全协议 sasl_mechanism="PLAIN", # 机制 ssl_context=context, sasl_plain_username='用户名', sasl_plain_password='密码', # api_version=(1, 10), group_id='xxx-mail-service-test' )TOPIC = 'mail_service'MAIL_HOST = 'smtp.qiye.163.com'MAIL_PORT = 465USER = 'xxx@163.cn'PASSWORD = 'xxx'PROJ_CONF = &#123; 'default': dict( from_addr='xxx@163.cn', to_addr=['xxx@163.cn'], title='报警系统' ), '新行情': dict( from_addr='xxx@163.cn', to_addr=['xxx@163.cn', 'xxx@163.cn'], title='xxx报警系统' ),&#125;logger = logging.Logger(__file__)logging_format = "[%(asctime)s]-[%(levelname)s] &lt;-%(filename)s-%(funcName)s-line %(lineno)s&gt;: %(message)s"datefmt = '%Y-%m-%d %H:%M:%S'log_config = dict( datefmt=datefmt, format=logging_format, level=logging.DEBUG,)stream_handler = StreamHandler()stream_handler.setLevel(logging.DEBUG)stream_handler.setFormatter( Formatter(logging_format, datefmt),)logger.addHandler(stream_handler)file_handler = FileHandler(LOG_FILE)file_handler.setLevel(logging.DEBUG)file_handler.setFormatter( Formatter(logging_format, datefmt),)logger.addHandler(file_handler)async def mail_consumer(topic, queue: asyncio.Queue, **kwargs): loop = asyncio.get_event_loop() consumer = AIOKafkaConsumer(topic, loop=loop, **kwargs) await consumer.start() async for msg in consumer: print(msg) value = msg.value logger.info(value) await queue.put(value)async def send(queue: asyncio.Queue): msg_obj = OrderedDict() while True: try: value = queue.get_nowait() data = json.loads(value) title = data.get('title') or PROJ_CONF.get('default').get('title') code = data.get('code') or uuid.uuid1().hex content = data.get('content') or json.dumps(data) one = msg_obj.setdefault(title, OrderedDict()) one.setdefault(code, content) except asyncio.QueueEmpty: if msg_obj: try: logger.info(msg_obj) for title, one in msg_obj.items(): contents = '\n'.join(list(one.values())) message = MIMEText(contents, 'plain', 'utf-8') message['Subject'] = PROJ_CONF.get(title, &#123;&#125;).get('title') or title from_addr = PROJ_CONF.get(title, &#123;&#125;).get('from_addr') or PROJ_CONF.get('default').get( 'from_addr') to_addrs = PROJ_CONF.get(title, &#123;&#125;).get('to_addr') or PROJ_CONF.get('default').get('to_addr') message['From'] = from_addr if isinstance(to_addrs, list): message['To'] = ','.join(to_addrs) elif isinstance(to_addrs, str): message['To'] = to_addrs else: logger.error('发送地址是非法的: %s'% to_addrs) smtp = SMTP_SSL( host=MAIL_HOST, port=MAIL_PORT, ) smtp.login( user=USER, password=PASSWORD, ) smtp.sendmail(from_addr, to_addrs, message.as_string()) logger.info('发送邮件成功') msg_obj.clear() except Exception as e: logger.error(e) logger.error(traceback.print_exc()) await asyncio.sleep(TIME_DELTA)async def main(): logger.info('服务启动') q = asyncio.Queue() await asyncio.gather( mail_consumer(topic=TOPIC, queue=q, **kafka_config), send(queue=q) )if __name__ == '__main__': asyncio.run(main()) (4)工具类-整合重复的报错(mail_producer.py)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import jsonimport sslimport osfrom kafka import KafkaProducerimport loggingclass MailProducer(object): def __init__(self, topic, title, send_mail=True, **kwargs, ): """ :param topic: kafka 邮件的topic :param title: 邮件按照title分类， 服务端会根据title找到对应收件人邮箱 :param kwargs: kafka的配置 """ self.topic = topic self.title = title self.send_mail = send_mail self.producer = KafkaProducer(**kwargs) self.kwargs = kwargs def send(self, content: str, code=None): """ :param content: :param code: :return: """ if self.send_mail: v = &#123; 'title': self.title, 'code': code, 'content': content &#125; value = json.dumps(v).encode('utf8') try: future = self.producer.send(self.topic, value) future.get(timeout=3) except Exception as e: producer = KafkaProducer(**self.kwargs) future = producer.send(self.topic, value) future.get(timeout=3)class MailHandler(logging.Handler): def __init__(self, topic, title=None, **kafka_config): logging.Handler.__init__(self) self.mail_producer = MailProducer(topic=topic, title=title, **kafka_config ) def emit(self, record): msg = self.format(record) self.mail_producer.send(msg)if __name__ == '__main__': context = ssl.SSLContext(ssl.PROTOCOL_SSLv23) context.verify_mode = ssl.CERT_REQUIRED cafile = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) + "/ca-cert" context.load_verify_locations(cafile) kafka_producer_config = dict( bootstrap_servers=[服务器IP:端口号], security_protocol='SASL_SSL', # 安全协议 sasl_mechanism="PLAIN", # 机制 ssl_context=context, sasl_plain_username='用户名', sasl_plain_password='密码', # api_version=(1, 10), ) # 发送邮件 mail_producer = MailProducer(topic='mail_service', title='', **kafka_config ) mail_producer.send('你好')]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>logger</tag>
        <tag>mail</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[本地SSH连接内网redis和MongoDB]]></title>
    <url>%2Fnanyi-air.github.io%2F2020%2F01%2F14%2F%E6%9C%AC%E5%9C%B0SSH%E8%BF%9E%E6%8E%A5%E5%86%85%E7%BD%91redis%E5%92%8CMongoDB%2F</url>
    <content type="text"><![CDATA[python 通过SSHTunnelForwarder隧道连接redis和MongoDB1.介绍该代码用于需要远程连接内网服务器上的redis或者MongoDB数据库时使用借助SSH搭建通道实现本地连接远程的数据库,方便本地调试 2.模块代码123456789101112131415161718192021server = SSHTunnelForwarder( ssh_address_or_host='xxx', # ssh地址 ssh_username='xxx', # ssh连接的用户名 ssh_password='xxx', # ssh连接的用户名 remote_bind_addresses=[('redis地址', 6379), ('mongoDB地址', MongoDB端口号)], local_bind_addresses=[('127.0.0.1', 10005), ('127.0.0.1', 13717)] ) if env == 'local': try: server.start() except Exception: pass redis_config = &#123; 'host': '127.0.0.1', 'port': 10005, 'db': 22, 'password': 'xxx', 'decode_responses': True &#125;]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>SSH</tag>
        <tag>redis</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[socket长连接通讯]]></title>
    <url>%2Fnanyi-air.github.io%2F2020%2F01%2F14%2Fsocket%E9%95%BF%E8%BF%9E%E6%8E%A5%E9%80%9A%E8%AE%AF%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[oss的应用]]></title>
    <url>%2Fnanyi-air.github.io%2F2020%2F01%2F14%2Foss%E7%9A%84%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[OSS使用1.获取bucketID,bucketKEY,URL等信息12345678910# ossOSS_BUCKET_NAME = 'xxx' # bucket_nameOSS_URL = 'xxx' # 外网内容查看URLfile_dir = '' # 存储路径# 对象存储AccessKeyID = 'xxx' # IDAccessKeySecret = 'xxx' # KEYOSS_ENDPOINT = "xxx" # OSS访问地址(记得ENDPOINT中加-internal是内网访问地址) 2.上传文件12345678910111213141516171819202122class OssBucket(object): def __init__(self, key, secret, bucket_name, endpoint): self.auth = oss2.Auth(key, secret) self.bucket = oss2.Bucket(self.auth, endpoint, bucket_name) # 上传内容(key为上传到oss的目录,content为内容) def upload_content(self, key, content): res = self.bucket.put_object(key, content) if res.status == 200: return key else: logger.error(f'上传阿里云失败 &#123;key&#125;') raise Exception # 上传文件(key为上传到oss的目录,file为文件路径) def upload_file(self, key, file): res = self.bucket.put_object_from_file(key, file) if res.status == 200: return key else: logger.error(f'上传阿里云失败 &#123;key&#125;') raise Exception 3.返回res结果根据返回的res.status是否为200判断是否上传成功]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>oss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx+uwsgi部署项目]]></title>
    <url>%2Fnanyi-air.github.io%2F2020%2F01%2F14%2Fnginx-uwsgi%E9%83%A8%E7%BD%B2%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[nginx+uwsgi+django在云服务器上部署项目uwsgi启动：uwsgi –ini uwsgi.ini 停止：uwsgi –stop uwsgi.pid 如果报错查看文件:vi uwsgi.log 停止如果还有剩余进程使用如下命令：pkill -f uwsgi -9 查看进程命令:ps aux | grep uwsgi nginx修改nginx配置文件：vi /usr/local/nginx/conf/nginx.conf 查看进程命令: ps aux | grep nginx 重启nginx服务：sudo /usr/local/nginx/sbin/nginx -s reload 停止nginx服务：sudo /usr/local/nginx/sbin/nginx -s stop 启动nginx服务：sudo /usr/local/nginx/sbin/nginx 查看nginx错误日志：vi /usr/local/nginx/logs/error.log vim查看后10行：tail -n 10 文件名 uwsgi配置 1.在项目目录下创建uwsgi.ini文件 12345678910111213141516171819[uwsgi]# 使用nginx连接（注意这里写的都是云服务器的内网IP）# socket=xxx:8000# 使用uwsgi部署http=xxx:8000# 项目目录注意不加引号chdir=/root/FanDong_Travel/TravelWeb/zhiqiTravel# 项目中wsgi.py文件路径wsgi-file=zhiqiTravel/wsgi.py# 进程数processes=4# 线程数threads=2# 存放进程编号的文件pidfile=uwsgi.pid# uwsgi日志daemonize=uwsgi.log# 虚拟环境virtualenv=/root/.virtualenvs/django2.1 2.连接远程服务器，然后将本地文件上传 3.启动uwsgi.ini文件 nginx配置静态服务器和反向代理-负载均衡 1.创建uwsgi2.ini文件，将端口号改为其他的例如8001之类的 2.启动uwsgi2.ini，使用ps aux | grep uwsgi查看是否启动了两个uwsgi 3.进入nginx的配置文件下修改配置/usr/local/nginx/conf/nginx.conf 1234567891011121314151617181920212223242526272829303132333435363738# 首先第一行修改为user root;需要root权限用户 # 配置负载均衡使用的 upstream zhiqiTravel&#123; server xxx:8000; server xxx:8001; &#125; server &#123; # 注意修改这里的端口号 listen 8000; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / &#123; # 将所有的参数转到uwsgi下 include uwsgi_params; # uwsgi的ip与端口 uwsgi_pass xxx:8000; # 配置负载均衡 # uwsgi_pass zhiqiTravel; # root html; # index index.html index.htm; &#125; # 配置静态文件 location /static &#123; autoindex on; # 注意如果是/目录使用root。其他使用alias alias /root/FanDong_Travel/TravelWeb/zhiqiTravel/static/; &#125; location /media &#123; alias /root/FanDong_Travel/TravelWeb/zhiqiTravel/media/; &#125; 4.注意将项目文件增加权限，chmod -R 777 项目文件：这里可能会产生一个问题 如果遇到静态文件加载不出来的问题，并且已经修改了nginx文件的情况下还是不行，那么就是你将文件放在root目录下，权限不够的问题，解决方案： 1.将项目移出root目录 2.给root文件设置最高权限sudo chmod -R 777 root 5.注意回到项目中manage.py文件所在的路径下执行： python manage.py collectstatic然后输入yes，将django项目中的静态文件分离出来 6.重启uwsgi和nginx即可]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>uwsgi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka接入获取数据]]></title>
    <url>%2Fnanyi-air.github.io%2F2020%2F01%2F14%2Fkafka%E6%8E%A5%E5%85%A5%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[kafka接入获取数据1.kafka配置1234567891011121314topic = '队列名'patch_topic = 'quote_test_zyan'group_id_1 = '分组ID'group_id_2 = '分组ID'group_id_3 = '分组ID'kafka_producer_config = dict( bootstrap_servers=[服务器IP:端口号], security_protocol='SASL_SSL', # 安全协议 sasl_mechanism="PLAIN", # 机制 ssl_context=context, sasl_plain_username='用户名', sasl_plain_password='密码', # api_version=(1, 10), ) 2.连接kafka获取数据12345678910111213141516171819async def consume(queue, group_id): loop = asyncio.get_event_loop() # 获取消费对象 consumer = AIOKafkaConsumer(Config.topic, loop=loop, **Config.kafka_producer_config, group_id=group_id) # 开始启动kafka await consumer.start() # 遍历获取到的数据,对数据进行简单处理以及筛选 async for msg in consumer: params = json.loads(msg.value.decode('utf8')) if not params.get('content'): continue data = params['content'] if data['trade_status'] not in ('OCALL', 'TRADE', 'ENDTR', 'SUSP', 'STOPT', 'BREAK'): continue # 向获取到的数据添加自己特定字段(根据自己需求添加也可以不添加) data.update(date=data['market_date']) # 加入异步 await queue.put(data)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[asyncio异步]]></title>
    <url>%2Fnanyi-air.github.io%2F2020%2F01%2F14%2Fasyncio%E5%BC%82%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[asyncio异步I/O应用1.介绍asyncio 是用来编写 并发 代码的库，使用 async/await 语法。 asyncio 被用作多个提供高性能 Python 异步框架的基础，包括网络和网站服务，数据库连接库，分布式任务队列等等。 asyncio 往往是构建 IO 密集型和高层级 结构化 网络代码的最佳选择。 使用场景:需要实时获取数据并对数据进行实时处理然后流出的情景. 2.应用(1)从kafka获取数据(获取数据具体步骤见另一篇文章)12345678async def first(): # 设置最大连接数,如果为0则表示无上限 queue = asyncio.Queue(maxsize=20000) await asyncio.gather( # 从kafka获取数据 consume(queue, Config.group_id_1), pusher_first(queue) ) (2)消费数据123456async def pusher_first(queue): while True: # 获取数据 data = await queue.get() # 异步框架来对数据进行处理,然后以字典的形式返回 processing_data(data, register_funcs=REGISTER_FUNCTIONS_FIRST) (3)对数据进行处理123456789101112131415161718192021222324252627282930313233343536def processing_data(data, register_funcs): # 获取当前传入数据的股票类型 own_funcs = register_funcs.get(data.get('secu_type'), []) # 根据stage排序执行,当前默认由小到大 for stage in sorted(own_funcs.keys()): fns = own_funcs[stage] for fn in fns: # 传入数据 fn(data)REGISTER_FUNCTIONS_FIRST = dict()def register_first(stage: int, types=None): """ :param types: 类别， 指数IND，股票SHARE，板块PLATE :param stage: 方法排序用的， stage 小的先执行 :return: None """ if types is None: types = ['SHARE', 'IND', 'PLATE'] if type(types) is str: types = [types] def decorator(fn): for t in types: REGISTER_FUNCTIONS_FIRST.setdefault(t, dict()).setdefault(stage, list()).append(fn) return fn return decorator# 给想要处理的数据方法添加装饰器,第一个参数为执行顺序由小到大(1-10),第二个参数为数据类型(根据自己需求定)@register_first(1, types='SHARE')def prepare_five(data): # 业务逻辑 pass 3.小结注意:该异步框架不能和celery连用,同为异步,数据不互通]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>异步IO</tag>
      </tags>
  </entry>
</search>
